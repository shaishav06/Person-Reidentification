{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbYlkcqCycb_"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install Keras\n"
      ],
      "id": "QbYlkcqCycb_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e5aa11d",
        "outputId": "e42507d4-4d26-4fb1-a041-b9a8498451f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "import sys\n",
        "from pycocotools.coco import COCO\n",
        "import urllib\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "4e5aa11d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "470j9xEALfaS"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/Market-1501-v15.09.15.zip"
      ],
      "id": "470j9xEALfaS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72c70f6f"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/Data Structure And Algorithm/data.zip','r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "id": "72c70f6f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "533cf43c",
        "outputId": "be5bdd58-dcff-4206-ae64-2df71f0b6e52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x128>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "from skimage import io\n",
        "img_path = \"/content/data/bounding_box_test/-1_c1s1_000401_03.jpg\"\n",
        "image = io.imread(img_path)\n",
        "cv2_imshow(image)\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()"
      ],
      "id": "533cf43c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjxMlbo30aXa",
        "outputId": "33a317eb-1073-412c-e76c-2bf61745da1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "AjxMlbo30aXa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff1d7777"
      },
      "outputs": [],
      "source": [
        "for dirname, _, filenames in os.walk('dataset'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname,filename))"
      ],
      "id": "ff1d7777"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "762ee350"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import torch"
      ],
      "id": "762ee350"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d88c501f"
      },
      "outputs": [],
      "source": [
        "# general purpose functions.Tensor to numpy convertot and vice-versa\n",
        "\n",
        "def get_image_id(imname):\n",
        "    return int(imname.split('_')[0])\n",
        "\n",
        "def get_image_cam(imname):\n",
        "    cam_str = imname.split('_')[1][1:]  # Extract the numeric part of the camera string\n",
        "    cam_str = ''.join(filter(str.isdigit, cam_str))  # Filter out non-numeric characters\n",
        "    return int(cam_str)\n",
        "\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    if torch.is_tensor(tensor):\n",
        "        return tensor.cpu().numpy()\n",
        "    elif type(tensor).__module__ != 'numpy':\n",
        "        raise ValueError(\"Cannot convert {} to numpy array \".format(type(tensor)))\n",
        "\n",
        "    return tensor\n"
      ],
      "id": "d88c501f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8df212c4"
      },
      "outputs": [],
      "source": [
        "def to_torch(ndarray):\n",
        "    if type(ndarray).__module__ == 'numpy':\n",
        "        return torch.from_numpy(ndarray)\n",
        "    elif not torch.is_tensor(ndarray):\n",
        "        raise ValueError(\"Cannot convert {} to torch tensor\".formar(type(ndarray)))\n",
        "\n",
        "    return ndarray"
      ],
      "id": "8df212c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "022e1fef"
      },
      "outputs": [],
      "source": [
        "# https://github.com/Cysu/open-reid/blob/master/reid/evaluation_metrics/ranking.py\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "def _unique_sample(ids_dict, num):\n",
        "    mask = np.zeros(num, dtype=np.bool)\n",
        "    for _, indices in ids_dict.items():\n",
        "        i = np.random.choice(indices)\n",
        "        mask[i] = True\n",
        "    return mask\n",
        "\n",
        "\n",
        "def cmc(distmat, query_ids=None, gallery_ids=None,\n",
        "        query_cams=None, gallery_cams=None, topk=100,\n",
        "        separate_camera_set=False,\n",
        "        single_gallery_shot=False,\n",
        "        first_match_break=False):\n",
        "    distmat = to_numpy(distmat)\n",
        "    m, n = distmat.shape\n",
        "    # Fill up default values\n",
        "    if query_ids is None:\n",
        "        query_ids = np.arange(m)\n",
        "    if gallery_ids is None:\n",
        "        gallery_ids = np.arange(n)\n",
        "    if query_cams is None:\n",
        "        query_cams = np.zeros(m).astype(np.int32)\n",
        "    if gallery_cams is None:\n",
        "        gallery_cams = np.ones(n).astype(np.int32)\n",
        "    # Ensure numpy array\n",
        "    query_ids = np.asarray(query_ids)\n",
        "    gallery_ids = np.asarray(gallery_ids)\n",
        "    query_cams = np.asarray(query_cams)\n",
        "    gallery_cams = np.asarray(gallery_cams)\n",
        "    # Sort and find correct matches\n",
        "    indices = np.argsort(distmat, axis=1)\n",
        "    matches = (gallery_ids[indices] == query_ids[:, np.newaxis])\n",
        "    # Compute CMC for each query\n",
        "    ret = np.zeros(topk)\n",
        "    num_valid_queries = 0\n",
        "    for i in range(m):\n",
        "        # Filter out the same id and same camera\n",
        "        valid = ((gallery_ids[indices[i]] != query_ids[i]) |\n",
        "                 (gallery_cams[indices[i]] != query_cams[i]))\n",
        "        if separate_camera_set:\n",
        "            # Filter out samples from same camera\n",
        "            valid &= (gallery_cams[indices[i]] != query_cams[i])\n",
        "        if not np.any(matches[i, valid]): continue\n",
        "        if single_gallery_shot:\n",
        "            repeat = 10\n",
        "            gids = gallery_ids[indices[i][valid]]\n",
        "            inds = np.where(valid)[0]\n",
        "            ids_dict = defaultdict(list)\n",
        "            for j, x in zip(inds, gids):\n",
        "                ids_dict[x].append(j)\n",
        "        else:\n",
        "            repeat = 1\n",
        "        for _ in range(repeat):\n",
        "            if single_gallery_shot:\n",
        "                # Randomly choose one instance for each id\n",
        "                sampled = (valid & _unique_sample(ids_dict, len(valid)))\n",
        "                index = np.nonzero(matches[i, sampled])[0]\n",
        "            else:\n",
        "                index = np.nonzero(matches[i, valid])[0]\n",
        "            delta = 1. / (len(index) * repeat)\n",
        "            for j, k in enumerate(index):\n",
        "                if k - j >= topk: break\n",
        "                if first_match_break:\n",
        "                    ret[k - j] += 1\n",
        "                    break\n",
        "                ret[k - j] += delta\n",
        "        num_valid_queries += 1\n",
        "    if num_valid_queries == 0:\n",
        "        raise RuntimeError(\"No valid query\")\n",
        "    return ret.cumsum() / num_valid_queries\n",
        "\n",
        "\n",
        "def mean_ap(distmat, query_ids=None, gallery_ids=None,\n",
        "            query_cams=None, gallery_cams=None):\n",
        "    distmat = to_numpy(distmat)\n",
        "    m, n = distmat.shape\n",
        "    # Fill up default values\n",
        "    if query_ids is None:\n",
        "        query_ids = np.arange(m)\n",
        "    if gallery_ids is None:\n",
        "        gallery_ids = np.arange(n)\n",
        "    if query_cams is None:\n",
        "        query_cams = np.zeros(m).astype(np.int32)\n",
        "    if gallery_cams is None:\n",
        "        gallery_cams = np.ones(n).astype(np.int32)\n",
        "    # Ensure numpy array\n",
        "    query_ids = np.asarray(query_ids)\n",
        "    gallery_ids = np.asarray(gallery_ids)\n",
        "    query_cams = np.asarray(query_cams)\n",
        "    gallery_cams = np.asarray(gallery_cams)\n",
        "    # Sort and find correct matches\n",
        "    indices = np.argsort(distmat, axis=1)\n",
        "    matches = (gallery_ids[indices] == query_ids[:, np.newaxis])\n",
        "    # Compute AP for each query\n",
        "    aps = []\n",
        "    for i in range(m):\n",
        "        # Filter out the same id and same camera\n",
        "        valid = ((gallery_ids[indices[i]] != query_ids[i]) |\n",
        "                 (gallery_cams[indices[i]] != query_cams[i]))\n",
        "        y_true = matches[i, valid]\n",
        "        y_score = -distmat[i][indices[i]][valid]\n",
        "        if not np.any(y_true): continue\n",
        "        aps.append(average_precision_score(y_true, y_score))\n",
        "    if len(aps) == 0:\n",
        "        raise RuntimeError(\"No valid query\")\n",
        "    return np.mean(aps)"
      ],
      "id": "022e1fef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3282a2c1",
        "outputId": "f7f1927a-fa35-41aa-86ae-24ea31256b1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "DEVICE = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ],
      "id": "3282a2c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99fb90a7"
      },
      "outputs": [],
      "source": [
        "# Trainig part\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "ROOT = './data'\n",
        "\n",
        "class Market1501TrainVal(Dataset):\n",
        "    def __init__(self,root,transform=None,batch_size=32,shuffle=True):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.split_train_val()\n",
        "\n",
        "        self.query_ptr = 0\n",
        "        self.query_done = False\n",
        "        self.query_ids = [get_image_id(c) for c in self.query_images]\n",
        "        self.query_cams = [get_image_cam(c) for c in self.query_images]\n",
        "\n",
        "\n",
        "        self.gallery_ptr = 0\n",
        "        self.gallery_done = False\n",
        "        self.query_ids = [get_image_id(c) for c in self.query_images]\n",
        "        self.query_cams = [get_image_cam(c) for c in self.query_images]\n",
        "\n",
        "\n",
        "        self.images = self.train_images\n",
        "        self.person_id_gallery = {}\n",
        "\n",
        "        for f_image in self.images:\n",
        "            person_id = int(f_image.split('_')[0])\n",
        "\n",
        "            if person_id not in self.person_id_gallery.keys():\n",
        "                self.person_id_gallery[person_id] = {\n",
        "                    'images': [],\n",
        "                    'ptr': 0\n",
        "                }\n",
        "            self.person_id_gallery[person_id]['images'].append(os.path.join(root, 'bounding_box_train', f_image))\n",
        "\n",
        "        self.images = [os.path.join(self.root,f_image) for f_image in self.images]\n",
        "        self.person_id_list = list(self.person_id_gallery.keys())\n",
        "\n",
        "        self.sample_ptr = 0\n",
        "        self.epoch_done = False\n",
        "        self.images_per_id = 4\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.person_id_list)\n",
        "\n",
        "\n",
        "    def split_train_val(self):\n",
        "        images = os.listdir(os.path.join(self.root,'bounding_box_train'))\n",
        "        images.sort()\n",
        "        images.pop()\n",
        "\n",
        "        images = np.array(images)\n",
        "        np.random.shuffle(images)\n",
        "\n",
        "        ids = np.array([get_image_id(f) for f in images])\n",
        "        cams = np.array([get_image_cam(f) for f in images])\n",
        "\n",
        "        unique_ids = np.unique(ids)\n",
        "        np.random.shuffle(unique_ids)\n",
        "\n",
        "        train_indices = []\n",
        "        query_indices = []\n",
        "        gallery_indices = []\n",
        "\n",
        "        for unique_id in unique_ids:\n",
        "            query_indices = []\n",
        "            indices = np.argwhere(unique_id == ids).flatten()\n",
        "\n",
        "            unique_cams = np.unique(cams[indices])\n",
        "\n",
        "            for unique_cam in unique_cams:\n",
        "                query_indices.append(indices[np.argwhere(unique_cam == cams[indices]).flatten()[0]])\n",
        "            gallery_indices_ = list(np.setdiff1d(indices, query_indices))\n",
        "\n",
        "\n",
        "            for query_index in query_indices:\n",
        "                if len(gallery_indices_) == 0 or len(np.argwhere(cams[query_index] != cams[gallery_indices_]).flatten())==0:\n",
        "                    query_indices.remove(query_index)\n",
        "                    gallery_indices_.append(query_index)\n",
        "\n",
        "            if len(query_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            query_indices += list(query_indices)\n",
        "            gallery_indices += list(gallery_indices_)\n",
        "\n",
        "            num_selected_ids = 0\n",
        "            num_selected_ids += 1\n",
        "            if num_selected_ids >= 100:\n",
        "                break\n",
        "\n",
        "        train_indices = np.setdiff1d(range(len(images)), np.hstack([query_indices, gallery_indices]))\n",
        "\n",
        "\n",
        "        self.train_images = images[train_indices]\n",
        "        self.query_images = images[query_indices]\n",
        "        self.gallery_images = images[gallery_indices]\n",
        "\n",
        "        self.train_images.sort()\n",
        "        self.query_images.sort()\n",
        "        self.gallery_images.sort()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def next_batch(self):\n",
        "        person_ids = self.person_id_list[self.sample_ptr: self.sample_ptr + self.batch_size]\n",
        "        labels = []\n",
        "        image_t = []\n",
        "\n",
        "        for id in person_ids:\n",
        "            for _ in range(self.images_per_id):\n",
        "                ptr = self.person_id_gallery[id]['ptr']\n",
        "                image = Image.open(self.person_id_gallery[id]['images'][ptr])\n",
        "                image_t.append(self.transform(image))\n",
        "                labels.append(id)\n",
        "                self.person_id_gallery[id]['ptr'] += 1\n",
        "                if self.person_id_gallery[id]['ptr'] >= len(self.person_id_gallery[id]['images']):\n",
        "                    self.person_id_gallery[id]['ptr'] = 0\n",
        "\n",
        "        image_t = torch.stack(image_t)\n",
        "        labels = torch.tensor(labels,dtype=torch.int32)\n",
        "\n",
        "        self.sample_ptr += self.batch_size\n",
        "        if self.sample_ptr >= len(self.person_id_list):\n",
        "            self.epoch_done = True\n",
        "\n",
        "        return image_t,labels\n",
        "\n",
        "    def next_batch_gallery(self):\n",
        "        images_t = []\n",
        "        labels_t = []\n",
        "\n",
        "        for i in range(self.gallery_ptr, min(self.gallery_ptr + self.batch_size,len(self.gallery_images))):\n",
        "            filename = self.gallery_images[i]\n",
        "            image = Image.open(os.path.join(self.root,'bounding_box_train',filename))\n",
        "            if self.transform is not None:\n",
        "                image = self.transform(image)\n",
        "            images_t.append(image)\n",
        "            labels_t.append(int(filename.split('_')[0]))\n",
        "\n",
        "        self.gallery_ptr += self.batch_size\n",
        "        if self.gallery_ptr >= len(self.gallery_images):\n",
        "            self.gallery_ptr = 0\n",
        "            self.gallery_done = True\n",
        "\n",
        "        images_t = torch.stack(images_t)\n",
        "        labels_t = torch.tensor(labels_t).long()\n",
        "\n",
        "        return images_t,labels_t\n",
        "\n",
        "    def start_over(self):\n",
        "        self.epoch_done = False\n",
        "        self.sample_ptr = 0\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.person_id_list)\n",
        "\n"
      ],
      "id": "99fb90a7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0064ea31"
      },
      "outputs": [],
      "source": [
        "# Testing Part\n",
        "\n",
        "class Market1501Test(Dataset):\n",
        "    def __init__(self,root,transform=None,batch_size=32):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        gallery_images_ = os.listdir(os.path.join(root,'bounding_box_test'))\n",
        "        gallery_images_.sort()\n",
        "        gallery_images_.pop()\n",
        "\n",
        "        self.gallery_images = []\n",
        "        for filename in gallery_images_:\n",
        "            if not filename.startswith('-1'):\n",
        "                self.gallery_images.append(filename)\n",
        "\n",
        "        self.gallery_ids = [get_image_id(c) for c in self.gallery_images]\n",
        "        self.gallery_cams = [get_image_cam(c) for c in self.gallery_images]\n",
        "\n",
        "        self.gallery_ptr = 0\n",
        "        self.gallery_done = False\n",
        "\n",
        "        self.query_images = os.listdir(os.path.join(root,'query'))\n",
        "        self.query_images.sort()\n",
        "        self.query_images.pop()\n",
        "\n",
        "        self.query_ids = [get_image_id(c) for c in self.query_images]\n",
        "        self.query_cams = [get_image_cam(c) for c in self.query_images]\n",
        "\n",
        "        self.query_ptr = 0\n",
        "        self.query_done = False\n",
        "\n",
        "    def next_batch_gallery(self):\n",
        "        images_t = []\n",
        "        labels_t = []\n",
        "\n",
        "        for i in range(self.gallery_ptr,min(self.gallery_ptr + self.batch_size,len(self.gallery(images)))):\n",
        "            filename = self.gallery_images[i]\n",
        "            image = Image.open(os.path.join(self.root,'bounding_box_test',filename))\n",
        "            if self.transform is not None:\n",
        "                image = self.transfrom(image)\n",
        "            images_t.append(image)\n",
        "            labels_t.append(int(filename.split('_')[0]))\n",
        "\n",
        "        self.gallery_ptr += self.batch_size\n",
        "        if self.gallery_ptr >= len(self.gallery_images):\n",
        "            self.gallery_ptr = 0\n",
        "            self.gallery_done = True\n",
        "\n",
        "        images_t = torch.stack(images_t)\n",
        "        labels_t = torch.tensor(labels_t).long()\n",
        "\n",
        "        return images_t,labels_t\n",
        "\n"
      ],
      "id": "0064ea31"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0592a42f"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "model_urls = {\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth'\n",
        "}\n",
        "\n",
        "def conv3x3(in_planes,out_planes,strid=1):\n",
        "    return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=stride,padding=1,bias= False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,inplanes,planes,stride=1,downsample=None):\n",
        "        super(BasicBlock,self).__init__()\n",
        "        self.conv1 = conc3x3(inplanes,planes,stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes,planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    class Bottleneck(nn.Module):\n",
        "        expansion = 4\n",
        "\n",
        "        def __init__(self,inplanes,planes,stride=1,downsample=None):\n",
        "            super(Bottleneck,self).__init__()\n",
        "            self.conv1 = nn.Conv2d(inplanes,planes,kernel_size=1,bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(planes)\n",
        "            self.conv2 = nn.Conv2d(planes,planes,kernel_size=3,stride=stride,padding=1,bias=False)\n",
        "            self.bn2 = nn.BatchNorm2d(planes)\n",
        "            self.conv2 = nn.Conv3d(planes,planes * 4,kernel_size=1,bias=False)\n",
        "            self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.downsample = downsample\n",
        "            self.stride = stride\n",
        "\n",
        "        def forward(self,x):\n",
        "            residual = x\n",
        "\n",
        "            out = self.conv1(x)\n",
        "            out = self.bn1(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            out = self.conv2(x)\n",
        "            out = self.bn2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            out = self.conv3(x)\n",
        "            out = self.bn3(out)\n",
        "\n",
        "            if self.downsample is not None:\n",
        "                residual = self.downsample(x)\n",
        "\n",
        "            out += residual\n",
        "            out = self.relu(out)\n",
        "            return out\n",
        "\n",
        "\n",
        "    class ResNet(nn.Module):\n",
        "\n",
        "        def __init__(self,block,layers,last_conv_stride=2):\n",
        "\n",
        "            self.inplanes = 64\n",
        "            super(ResNet,self).__init__()\n",
        "            self.conv1 = nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size=7,stride=2,padding=3,bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.layer1 = self._make_layer(block,64,layers[0])\n",
        "            self.layer2 = self._make_layer(block,128,layers[1],stride=2)\n",
        "            self.layer3 = self._make_layer(block,256,layers[2],stride=2)\n",
        "            self.layer4 = self._make_layer(block,512,layers[3],stride=last_conv_stride)\n",
        "\n",
        "            for m in self.modules():\n",
        "                if isinstance(m,nn.Conv2d):\n",
        "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                    m.weight.data.normal_(0,math.sqrt(2. / n))\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    m.weight.data.fill_(1)\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "        def _make_layer(self,block,planes,blocks,stride=1):\n",
        "            downsample = None\n",
        "            if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "                downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes,planes * block.expansion,kernel_size=1, stride=stride,bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "                )\n",
        "            layers = []\n",
        "            layers.append(block(self.inplanes,planes,stride,downsample))\n",
        "            self.inplanes = planes * block.expansion\n",
        "            for i in range(1,blocks):\n",
        "                layers.append(block(self.inplanes,planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        def forward(self,x):\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.maxpool(x)\n",
        "\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.layer3(x)\n",
        "            x = self.layer4(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "        def remove_fc(state_dict):\n",
        "            new_state_dict = {}\n",
        "\n",
        "            for key,value in state_dict.items():\n",
        "                if not key.startwith('fc.'):\n",
        "                    new_state_dict[key] = value\n",
        "\n",
        "            return new_state_dict\n",
        "\n",
        "        def resnet50(pretrained=False, **kwargs):\n",
        "            model = ResNet(Bottleneck,[3,4,6,3], **kwargs)\n",
        "            if pretrained:\n",
        "                model.load_state_dict(remove_fc(model_zoo,load_url(model_urls['resnet50'])))\n",
        "            return model"
      ],
      "id": "0592a42f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fba7760"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self,last_conv_stride=2,local_conv_out_channels=128):\n",
        "        super(Model, self).__init__()\n",
        "        self.base = resnet50(pretrained=True)\n",
        "        modules = list(self.base.children())[:-2]\n",
        "        modules[-1][0].conv2.stride = (1, 1)\n",
        "        modules[-1][0].downsample[0].stride = (1, 1)\n",
        "        self.base = nn.Sequential(*modules)\n",
        "        self.local_feat_extractor = nn.Sequential(\n",
        "        nn.Conv2d(2048,local_conv_out_channels,1),\n",
        "        nn.BatchNorm2d(local_conv_out_channels),\n",
        "        nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self,x):\n",
        "        feat = self.base(x)\n",
        "        global_feat = F.avg_pool2d(feat, feat.size()[2:])\n",
        "        global_feat = global_feat.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "        local_feat = torch.mean(feat,-1,keepdim=True)\n",
        "        local_feat = self.local_feat_extractor(local_feat)\n",
        "\n",
        "        local_feat = local_feat.squeeze(-1).permute(0,2,1)\n",
        "\n",
        "        return global_feat, local_feat\n",
        "\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torchvision.models import resnet50\n",
        "\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     def __init__(self,last_conv_stride=2,local_conv_out_channels=128):\n",
        "#         super(Model,self).__init__()\n",
        "#         self.base = resnet50(pretrained=True)\n",
        "#         # self.base = resnet50(pretrained=True, last_conv_stride=last_conv_stride)\n",
        "\n",
        "#         self.local_feat_extractor = nn.Sequential(\n",
        "#             nn.Conv2d(2048,local_conv_out_channels,1),\n",
        "#             nn.BatchNorm2d(local_conv_out_channels),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#             )\n",
        "\n",
        "#     def forward(self,x):\n",
        "#       # Shape [N, C, H, W]\n",
        "#         feat = self.base(x)\n",
        "#         global_feat = F.avg_pool2d(feat,feat.size()[2:])\n",
        "#         #shape [N,C,H,1]\n",
        "#         global_feat = global_feat.view(feat.size(0), -1)\n",
        "\n",
        "#         #shape [N,C,H,1]\n",
        "#         local_feat = torch.mean(feat,-1,keepdim=True)\n",
        "#         local_feat = self.local_feat_extractor(local_feat)\n",
        "\n",
        "#         #shape [N, H, c]\n",
        "#         local_feat = local_feat.squeeze(-1).permute(0,2,1)\n",
        "\n",
        "#         return global_feat, local_feat\n"
      ],
      "id": "6fba7760"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2edb5c1"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class TripletLoss(object):\n",
        "    def __init__(self, margin=None):\n",
        "        self.margin = margin\n",
        "        if margin is not None:\n",
        "            self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
        "        else:\n",
        "            self.ranking_loss = nn.SoftMarginLoss()\n",
        "\n",
        "    def __call__(self, dist_ap,dist_an):\n",
        "        if dist_an.numel() == 0:\n",
        "            raise ValueError(\"dist_an is empty. There might be an issue in your code.\")\n",
        "\n",
        "        y = torch.ones_like(dist_an)\n",
        "\n",
        "        # y = Variable(dist_an.data.new().resize_as(dist_an.data).fill_(1))\n",
        "        if self.margin is not None:\n",
        "            loss = self.ranking_loss(dist_an,dist_ap,y)\n",
        "        else:\n",
        "            loss = self.ranking_loss(dist_an - dist_ap,y)\n",
        "        return loss"
      ],
      "id": "a2edb5c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51c81619"
      },
      "outputs": [],
      "source": [
        "# Euclidean distance function\n",
        "\n",
        "def euclidean_dist(x,y):\n",
        "    m,n = x.size(0),y.size(0)\n",
        "    xx = torch.pow(x,2).sum(1,keepdim=True).expand(m,n)\n",
        "    yy = torch.pow(y,2).sum(1,keepdim=True).expand(n,m).t()\n",
        "    dist = xx + yy\n",
        "    # dist = torch.addmm(1, dist, -2, x, y.t())\n",
        "    dist.addmm(1,-2,x,y.t())\n",
        "    dist = dist.clamp(min=1e-12).sqrt()\n",
        "    return dist\n",
        "\n",
        "# def euclidean_dist(x, y):\n",
        "#     m, n = x.size(0), y.size(0)\n",
        "#     xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
        "#     yy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
        "#     dist = xx + yy\n",
        "#     dist = torch.addmm(1, dist, -2, x, y.t())  # Use addmm without in-place operation\n",
        "#     dist = dist.clamp(min=1e-12).sqrt()\n",
        "#     return dist\n",
        "\n",
        "# def shortest_dist(dist_mat):\n",
        "#     m, n = dist_mat.size()[:2]\n",
        "#     dist = [[0 for _ in range(n)] for _ in range(m)]\n",
        "#     for i in range(m):\n",
        "#         for j in range(n):\n",
        "#             if (i == 0) and (j == 0):\n",
        "#                 dist[i][j] = dist_mat[i, j]\n",
        "#             elif (i == 0) and (j > 0):\n",
        "#                 dist[i][j] = dist[i][j-1] + dist_mat[i,j]\n",
        "#             elif (i > 0) and (j == 0):\n",
        "#                 dist[i][j] = dist[i - 1][j] + dist_mat[i, j]\n",
        "#             else:\n",
        "#                 dist[i][j] = torch.min(dist[i - 1][j],dist[i][j - 1])+ dist_mat[i][j]\n",
        "#         dist = dist[-1][-1]\n",
        "#         return dist\n",
        "\n",
        "def shortest_dist(dist_mat):\n",
        "    m, n = dist_mat.size()[:2]\n",
        "    dist = [[0 for _ in range(n)] for _ in range(m)]\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            if (i == 0) and (j == 0):\n",
        "                dist[i][j] = dist_mat[i, j]\n",
        "            elif (i == 0) and (j > 0):\n",
        "                dist[i][j] = dist[i][j-1] + dist_mat[i, j]\n",
        "            elif (i > 0) and (j == 0):\n",
        "                dist[i][j] = dist[i - 1][j] + dist_mat[i, j]\n",
        "            else:\n",
        "                dist[i][j] = torch.min(dist[i - 1][j], dist[i][j - 1]) + dist_mat[i][j]\n",
        "\n",
        "    # Convert the dist list to a tensor\n",
        "    dist = torch.tensor(dist, dtype=dist_mat.dtype, device=dist_mat.device)\n",
        "    return dist[-1, -1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def local_dist(x,y):\n",
        "#     M, m, d = x.size()\n",
        "#     N, n, d = y.size()\n",
        "#     x = x.contiguous().view(M * m,d)\n",
        "#     y = y.contiguous().view(N * n,d)\n",
        "\n",
        "#     dist_mat = euclidean_dist(x,y)\n",
        "#     dist_mat = (torch.exp(dist_mat)-1.)/(torch.exp(dist_mat)+ 1.)\n",
        "\n",
        "#     dist_mat = dist_mat.contiguous().view(M,m,N,n).permute(1,3,0,2)\n",
        "\n",
        "#     dist_mat = shortest_dist(dist_mat)\n",
        "\n",
        "#     return dist_mat\n",
        "\n",
        "# def local_dist(x, y):\n",
        "#     M, m, d = x.size()\n",
        "#     N, n, d = y.size()\n",
        "#     x = x.contiguous().view(M * m, d)\n",
        "#     y = y.contiguous().view(N * n, d)\n",
        "\n",
        "#     dist_mat = euclidean_dist(x, y)\n",
        "#     dist_mat = (torch.exp(dist_mat) - 1.) / (torch.exp(dist_mat) + 1.)\n",
        "\n",
        "#     dist_mat = dist_mat.contiguous().view(M, m, N, n).permute(1, 3, 0, 2)\n",
        "\n",
        "#     # Compute the shortest distance directly without using the shortest_dist function\n",
        "#     dist = dist_mat.clone()\n",
        "#     for i in range(m):\n",
        "#         for j in range(n):\n",
        "#             if (i > 0) and (j > 0):\n",
        "#                 dist[i, j] += torch.min(dist[i - 1, j], dist[i, j - 1])\n",
        "#             elif (i == 0) and (j > 0):\n",
        "#                 dist[i, j] += dist[i, j - 1]\n",
        "#             elif (i > 0) and (j == 0):\n",
        "#                 dist[i, j] += dist[i - 1, j]\n",
        "\n",
        "#     return dist[-1, -1]\n",
        "\n",
        "def local_dist(x, y):\n",
        "    M, m, d = x.size()\n",
        "    N, n, d = y.size()\n",
        "    x = x.contiguous().view(M * m, d)\n",
        "    y = y.contiguous().view(N * n, d)\n",
        "\n",
        "    dist_mat = euclidean_dist(x, y)\n",
        "    dist_mat = (torch.exp(dist_mat) - 1.) / (torch.exp(dist_mat) + 1.)\n",
        "\n",
        "    dist_mat = dist_mat.contiguous().view(M, m, N, n).permute(1, 3, 0, 2)\n",
        "\n",
        "    # Compute the shortest distance directly without using the shortest_dist function\n",
        "    dist = dist_mat.clone()  # Create a new tensor to accumulate values\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            if (i > 0) and (j > 0):\n",
        "                dist[i, j] = dist_mat[i, j] + torch.min(dist[i - 1, j], dist[i, j - 1])\n",
        "            elif (i == 0) and (j > 0):\n",
        "                dist[i, j] = dist_mat[i, j] + dist[i, j - 1]\n",
        "            elif (i > 0) and (j == 0):\n",
        "                dist[i, j] = dist_mat[i, j] + dist[i - 1, j]\n",
        "\n",
        "    return dist[-1, -1]\n",
        "\n",
        "# def hard_example_mining(dist_mat, labels):\n",
        "#     assert len(dist_mat.size()) == 2\n",
        "#     assert dist_mat.size(0) == dist_mat.size(1)\n",
        "#     N = dist_mat.size(0)\n",
        "\n",
        "#     is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\n",
        "#     is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\n",
        "\n",
        "#     dist_ap, relative_p_inds = torch.max(\n",
        "#         dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\n",
        "\n",
        "#     dist_ap = dist_ap.squeeze(1)\n",
        "\n",
        "#     # Fix the variable assignment here\n",
        "#     dist_an, relative_n_inds = torch.min(\n",
        "#         dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\n",
        "\n",
        "#     dist_an = dist_an.squeeze(1)\n",
        "\n",
        "#     return dist_ap, dist_an\n",
        "def hard_example_mining(dist_mat, labels):\n",
        "    assert len(dist_mat.size()) == 2\n",
        "    assert dist_mat.size(0) == dist_mat.size(1)\n",
        "    N = dist_mat.size(0)\n",
        "\n",
        "    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\n",
        "    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\n",
        "\n",
        "    dist_ap, relative_p_inds = torch.max(\n",
        "        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\n",
        "\n",
        "    dist_ap = dist_ap.squeeze(1)\n",
        "\n",
        "    # Set a default value for dist_an\n",
        "    dist_an = torch.zeros_like(dist_ap)\n",
        "\n",
        "    # Update dist_an if there are negative samples\n",
        "    if torch.any(is_neg):\n",
        "        dist_an, relative_n_inds = torch.min(\n",
        "            dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\n",
        "\n",
        "        dist_an = dist_an.squeeze(1)\n",
        "\n",
        "    return dist_ap, dist_an\n",
        "\n",
        "\n",
        "\n",
        "# def hard_example_mining(dist_mat,labels):\n",
        "#     assert len(dist_mat.size()) == 2\n",
        "#     assert dist_mat.size(0) == dist_mat.size(1)\n",
        "#     N = dist_mat.size(0)\n",
        "\n",
        "#     is_pos = labels.expand(N,N).eq(labels.expand(N,N).t())\n",
        "#     is_neg = labels.expand(N,N).ne(labels.expand(N,N).t())\n",
        "\n",
        "#     dist_ap,relative_p_inds = torch.max(\n",
        "#     dist_mat[is_pos].contiguous().view(N,-1),1,keepdim=True)\n",
        "\n",
        "#     dist_ap = dist_ap.squeeze(1)\n",
        "#     dist_an = dist_an.squeeze(1)\n",
        "\n",
        "#     return dist_ap,dist_an\n",
        "\n",
        "def global_loss(tri_loss,global_feat,labels):\n",
        "    dist_mat = euclidean_dist(global_feat,global_feat)\n",
        "    dist_ap,dist_an = hard_example_mining(dist_mat,labels)\n",
        "    loss = tri_loss(dist_ap,dist_an)\n",
        "\n",
        "    return loss, dist_ap, dist_an\n",
        "\n",
        "def local_loss(tri_loss,local_feat,labels):\n",
        "    dist_mat = local_dist(local_feat,local_feat)\n",
        "    # dist_mat = 10\n",
        "    dist_ap,dist_an = hard_example_mining(dist_mat,labels)\n",
        "    loss = tri_loss(dist_ap,dist_an)\n",
        "\n",
        "    return loss,dist_ap,dist_an\n"
      ],
      "id": "51c81619"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d621a5a"
      },
      "outputs": [],
      "source": [
        "# validation\n",
        "\n",
        "def valid(model, dataset):\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    total_query_features = []\n",
        "    total_query_labels = []\n",
        "\n",
        "    query_feat = []\n",
        "    dataset.query_done = False\n",
        "    dataset.query_ptr = 0\n",
        "\n",
        "    while not dataset.query_done:\n",
        "        image_t,labels_t = dataset.next_batch_query()\n",
        "        if images_t is None or labels_t is None:\n",
        "            break\n",
        "\n",
        "        images_t = images_t.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "          feat, _ = model(images_t)\n",
        "\n",
        "        total_query_features.append(feat)\n",
        "        tota;_query_labels.append(labels_t)\n",
        "\n",
        "        query_feat.append(feat.cpu().detach())\n",
        "\n",
        "    query_feat = torch.cat(query_feat)\n",
        "\n",
        "    gallery_feat = []\n",
        "    dataset.gallery_done = False\n",
        "    dataset.gallery_ptr = 0\n",
        "\n",
        "    while not dataset.gallery_done:\n",
        "        images_t,labels_t = dataset.next_batch_gallery()\n",
        "        images_t = images_t.to(DEVICE)\n",
        "        feat, _ = model(images_t)\n",
        "        query_feat.append(feat.cpu().detach())\n",
        "\n",
        "    gallery_feat = torch.cat(gallery_feat)\n",
        "\n",
        "    # calculate distance\n",
        "    dist_mat = euclidean_dist(query_feat,gallery_feat)\n",
        "\n",
        "    cmc_score = cmc(dist_mat,dataset.query_ids,dataset.gallery_ids,dataset.query_cams,dataset.gallery_cams,topk=5,separate_camera_set=False,single_gallery_shot=False,first_match_break=True)\n",
        "\n",
        "    # calculate average precision\n",
        "    mAP = mean_ap(dist_mat,dataset.query_ids,dataset.gallery_ids,dataset.query_cams,dataset.gallery_cams)\n",
        "\n",
        "    model.train()\n",
        "    return mAP,cmc_score"
      ],
      "id": "0d621a5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b70fd1f0"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "\n",
        "n_epochs = 2\n",
        "\n",
        "g_margin = 0.3\n",
        "g_loss_weight = 0.5\n",
        "\n",
        "l_margin = 0.3\n",
        "l_loss_weight = 0.5\n",
        "\n",
        "batch_size = 32"
      ],
      "id": "b70fd1f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86d8ddb2",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Prepare for Training\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Resize([256,128]),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "market1501 = Market1501TrainVal(ROOT,transform,batch_size=batch_size)"
      ],
      "id": "86d8ddb2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0dc7504",
        "outputId": "daaa85d3-6ca0-4069-9d62-9831c67fd40c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 57.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Adam Optimizer Use\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "model = Model(last_conv_stride=1)\n",
        "cur_epoch = 0\n",
        "\n",
        "g_tri_loss = TripletLoss(g_margin)\n",
        "l_tri_loss = TripletLoss(l_margin)\n",
        "optimizer = Adam(model.parameters(),lr=2e-4,weight_decay=0.0005)\n",
        "scheduler = StepLR(optimizer,step_size=100,gamma=0.1)\n"
      ],
      "id": "c0dc7504"
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "\n",
        "print(f'number of person-ids for training: {len(market1501.person_id_list)}')\n",
        "print(f'number of training images: {len(market1501.train_images)}')\n",
        "print(f'number of query images for validation: {len(market1501.query_images)}')\n",
        "print(f'number of gallery images for validation: {len(market1501.gallery_images)}')\n",
        "\n",
        "model.train()\n",
        "\n",
        "\n",
        "for epoch in range(cur_epoch,n_epochs):\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    print(f'epoch:{epoch}')\n",
        "    market1501.start_over()\n",
        "\n",
        "    total_loss = []\n",
        "    total_g_loss = []\n",
        "    total_g_prec = []\n",
        "    total_g_sm = []\n",
        "    total_g_dist_ap = []\n",
        "    total_g_dist_an = []\n",
        "\n",
        "    total_l_loss = []\n",
        "    total_l_prec = []\n",
        "    total_l_sm = []\n",
        "    total_l_dist_ap = []\n",
        "    total_l_dist_an = []\n",
        "\n",
        "    iter = 0\n",
        "    t_now = time.time()\n",
        "\n",
        "\n",
        "    while not market1501.epoch_done:\n",
        "        images_t, labels_t = market1501.next_batch()\n",
        "        images_t = Variable(images_t)\n",
        "\n",
        "        g_feat, l_feat = model(images_t)\n",
        "\n",
        "        g_loss,g_dist_ap,g_dist_an = global_loss(g_tri_loss,g_feat,labels_t)\n",
        "        l_loss,l_dist_ap, l_dist_an = local_loss(l_tri_loss, l_feat ,labels_t)\n",
        "\n",
        "        loss = g_loss_weight * g_loss + l_loss_weight * l_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        total_loss.append(loss.item())\n",
        "\n",
        "        total_g_loss.append(g_loss.item())\n",
        "        total_g_prec.append((g_dist_an > g_dist_ap).data.float().mean().item())\n",
        "        total_g_sm.append((g_dist_an > g_dist_ap + g_margin).data.float().mean().item())\n",
        "        total_g_dist_ap.append(g_dist_ap.data.float().mean().item())\n",
        "        total_g_dist_an.append(g_dist_an.data.float().mean().item())\n",
        "\n",
        "        total_l_loss.append(l_loss.item())\n",
        "        total_l_prec.append((l_dist_an > l_dist_ap).data.float().mean().item())\n",
        "        total_l_sm.append((l_dist_an > l_dist_ap + l_margin).data.float().mean().item())\n",
        "        total_l_dist_ap.append(l_dist_ap.data.float().mean().item())\n",
        "        total_l_dist_an.append(l_dist_an.data.float().mean().item())\n",
        "\n",
        "        iter +=1\n",
        "    print(f'in {(time.time() - t_now)}s')\n",
        "    print(f'mean_loss = {np.mean(total_loss)}, mean_g_loss = {np.mean(total_g_loss)}, mean_l_loss = {np.mean(total_l_loss)}')\n",
        "    print(f'mean_g_prec = {np.mean(total_g_prec)}, mean_l_prec = {np.mean(total_l_prec)}')\n",
        "    print(f'mean_g_sm = {np.mean(total_g_sm)}, mean_l_sm = {np.mean(total_l_sm)}')\n",
        "    print(f'mean_g_dist_ap = {np.mean(total_g_dist_ap)}, mean_l_dist_ap = {np.mean(total_l_dist_ap)}')\n",
        "    # print(f'mean_g_dist_ap = {np.mean(total_dist_ap)}, mean_l_dist_ap = {np.mean(total_l_dist_ap)}')\n",
        "    print(f'mean_g_dist_an = {np.mean(total_g_dist_an)}, mean_l_dist_an = {np.mean(total_l_dist_an)}')\n",
        "    # print(f'mean_g_dist_an = {np.mean(total_dist_an)}, mean_l_dist_an = {np.mean(total_l_dist_an)}')\n",
        "    print(f'learning_rate = {scheduler.get_last_lr()}')\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if(epoch + 1) % 2 == 0:\n",
        "        mAP,cmc_score = valid(model,market1501)\n",
        "        print(f'-----------------------------------')\n",
        "        print(f'validation: mAP={mAP}, rank1={cmc_score[0]},rank5={cmc_score[4]}')\n",
        "        print(f'-----------------------------------')\n",
        "\n",
        "    if(epoch + 1) % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        },f'checkpoint_s1_{epoch+1}.pth')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "w4eTAVvokoyM",
        "outputId": "b571e082-5c53-470d-c8da-82ab0a4b575a"
      },
      "id": "w4eTAVvokoyM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of person-ids for training: 10\n",
            "number of training images: 43\n",
            "number of query images for validation: 2\n",
            "number of gallery images for validation: 155\n",
            "epoch:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "<ipython-input-16-18001ddd3432>:9: UserWarning: This overload of addmm is deprecated:\n",
            "\taddmm(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddmm(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
            "  dist.addmm(1,-2,x,y.t())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in 10.560121774673462s\n",
            "mean_loss = 1.9257081747055054, mean_g_loss = 3.546273708343506, mean_l_loss = 0.3051425516605377\n",
            "mean_g_prec = 0.0, mean_l_prec = 0.0\n",
            "mean_g_sm = 0.0, mean_l_sm = 0.0\n",
            "mean_g_dist_ap = 34.86772537231445, mean_l_dist_ap = 30.996337890625\n",
            "mean_g_dist_an = 31.621450424194336, mean_l_dist_an = 30.991199493408203\n",
            "learning_rate = [0.0002]\n",
            "epoch:1\n",
            "in 9.446252346038818s\n",
            "mean_loss = 1.9568302631378174, mean_g_loss = 3.608870267868042, mean_l_loss = 0.304790198802948\n",
            "mean_g_prec = 0.0, mean_l_prec = 0.0\n",
            "mean_g_sm = 0.0, mean_l_sm = 0.0\n",
            "mean_g_dist_ap = 35.076385498046875, mean_l_dist_ap = 30.99624252319336\n",
            "mean_g_dist_an = 31.76751708984375, mean_l_dist_an = 30.9914493560791\n",
            "learning_rate = [0.0002]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ad3cc43e7161>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mmAP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarket1501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'-----------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'validation: mAP={mAP}, rank1={cmc_score[0]},rank5={cmc_score[4]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-0ee84fb6e1d1>\u001b[0m in \u001b[0;36mvalid\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimage_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimages_t\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlabels_t\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Market1501TrainVal' object has no attribute 'next_batch_query'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "046f9859",
        "outputId": "f3f97228-c2ae-4f09-e908-868547015c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of person-ids for training: 10\n",
            "number of training images: 40\n",
            "number of query images for validation: 8\n",
            "number of gallery images for validation: 155\n",
            "epoch:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-38028dbca18e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# image_t, labels_t = market1501.get_query_batch()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# image_t, labels_t = market1501.get_validation_query_batch()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mquery_image_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarket1501\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_validation_query_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mgallery_image_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_labels_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarket1501\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_validation_gallery_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Market1501TrainVal' object has no attribute 'get_validation_query_batch'"
          ]
        }
      ],
      "source": [
        "# # Start training\n",
        "\n",
        "# import numpy as np\n",
        "# from torch.autograd import Variable\n",
        "# import time\n",
        "\n",
        "# print(f'number of person-ids for training: {len(market1501.person_id_list)}')\n",
        "# print(f'number of training images: {len(market1501.train_images)}')\n",
        "# print(f'number of query images for validation: {len(market1501.query_images)}')\n",
        "# print(f'number of gallery images for validation: {len(market1501.gallery_images)}')\n",
        "\n",
        "# model.train()\n",
        "\n",
        "# for epoch in range(cur_epoch,n_epochs):\n",
        "#     torch.autograd.set_detect_anomaly(True)\n",
        "#     print(f'epoch:{epoch}')\n",
        "#     market1501.start_over()\n",
        "\n",
        "#     total_loss = []\n",
        "#     total_g_loss = []\n",
        "#     total_g_prec = []\n",
        "#     total_g_sm = []\n",
        "#     total_g_dist_ap = []\n",
        "#     total_g_dist_an = []\n",
        "\n",
        "#     total_l_loss = []\n",
        "#     total_l_prec = []\n",
        "#     total_l_sm = []\n",
        "#     total_l_dist_ap = []\n",
        "#     total_l_dist_an = []\n",
        "\n",
        "#     iter = 0\n",
        "#     t_now = time.time()\n",
        "#     while not market1501.epoch_done:\n",
        "#     # while not market1501.query_done:\n",
        "\n",
        "#         # images_t, labels_t = market1501.next_batch()\n",
        "#         # image_t, labels_t = market1501.get_query_batch()\n",
        "#         # image_t, labels_t = market1501.get_validation_query_batch()\n",
        "#         query_image_t, query_labels_t = market1501.get_validation_query_batch()\n",
        "#         gallery_image_t, gallery_labels_t = market1501.get_validation_gallery_batch()\n",
        "\n",
        "#         query_images_t = query_image_t.to(DEVICE)\n",
        "#         gallery_images_t = gallery_image_t.to(DEVICE)\n",
        "\n",
        "#         query_feat, _ = model(query_images_t)\n",
        "#         gallery_feat, _ = model(gallery_images_t)\n",
        "\n",
        "#         # images_t = Variable(images_t)\n",
        "\n",
        "#         # g_feat, l_feat = model(images_t)\n",
        "#         g_loss,g_dist_ap,g_dist_an = global_loss(g_tri_loss,gallery_feat,query_labels_t)\n",
        "#         l_loss,l_dist_ap, l_dist_an = local_loss(l_tri_loss, _ ,query_labels_t)\n",
        "\n",
        "\n",
        "#         # g_loss,g_dist_ap,g_dist_an = global_loss(g_tri_loss,g_feat,labels_t)\n",
        "#         # l_loss,l_dist_ap, l_dist_an = local_loss(l_tri_loss,l_feat,labels_t)\n",
        "\n",
        "#         loss = g_loss_weight * g_loss + l_loss_weight * l_loss\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         # loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "#         total_loss.append(loss.item())\n",
        "\n",
        "#         total_g_loss.append(g_loss.item())\n",
        "#         total_g_prec.append((g_dist_an > g_dist_ap).data.float().mean().item())\n",
        "#         total_g_sm.append((g_dist_an > g_dist_ap + g_margin).data.float().mean().item())\n",
        "#         total_g_dist_ap.append(g_dist_ap.data.float().mean().item())\n",
        "#         total_g_dist_an.append(g_dist_an.data.float().mean().item())\n",
        "\n",
        "#         total_l_loss.append(l_loss.item())\n",
        "#         total_l_prec.append((l_dist_an > l_dist_ap).data.float().mean().item())\n",
        "#         total_l_sm.append((l_dist_an > l_dist_ap + l_margin).data.float().mean().item())\n",
        "#         total_l_dist_ap.append(l_dist_ap.data.float().mean().item())\n",
        "#         total_l_dist_an.append(l_dist_an.data.float().mean().item())\n",
        "\n",
        "#         iter +=1\n",
        "#     print(f'in {(time.time() - t_now)}s')\n",
        "#     print(f'mean_loss = {np.mean(total_loss)}, mean_g_loss = {np.mean(total_g_loss)}, mean_l_loss = {np.mean(total_l_loss)}')\n",
        "#     print(f'mean_g_prec = {np.mean(total_g_prec)}, mean_l_prec = {np.mean(total_l_prec)}')\n",
        "#     print(f'mean_g_sm = {np.mean(total_g_sm)}, mean_l_sm = {np.mean(total_l_sm)}')\n",
        "#     print(f'mean_g_dist_ap = {np.mean(total_g_dist_ap)}, mean_l_dist_ap = {np.mean(total_l_dist_ap)}')\n",
        "#     # print(f'mean_g_dist_ap = {np.mean(total_dist_ap)}, mean_l_dist_ap = {np.mean(total_l_dist_ap)}')\n",
        "#     print(f'mean_g_dist_an = {np.mean(total_g_dist_an)}, mean_l_dist_an = {np.mean(total_l_dist_an)}')\n",
        "#     # print(f'mean_g_dist_an = {np.mean(total_dist_an)}, mean_l_dist_an = {np.mean(total_l_dist_an)}')\n",
        "#     print(f'learning_rate = {scheduler.get_last_lr()}')\n",
        "\n",
        "#     scheduler.step()\n",
        "\n",
        "#     if(epoch + 1) % 2 == 0:\n",
        "#         mAP,cmc_score = valid(model,market1501)\n",
        "#         print(f'-----------------------------------')\n",
        "#         print(f'validation: mAP={mAP}, rank1={cmc_score[0]},rank5={cmc_score[4]}')\n",
        "#         print(f'-----------------------------------')\n",
        "\n",
        "#     if(epoch + 1) % 5 == 0:\n",
        "#         torch.save({\n",
        "#             'epoch': epoch + 1,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#         },f'checkpoint_s1_{epoch+1}.pth')\n",
        "\n"
      ],
      "id": "046f9859"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a48dc5f9"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model_weight.pth')\n"
      ],
      "id": "a48dc5f9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeiDD321TjFa"
      },
      "outputs": [],
      "source": [
        "market1501_test = Market1501Test(ROOT,transfrom,batch_size=32)"
      ],
      "id": "zeiDD321TjFa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwgV033gTmXp"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "query_feat = []\n",
        "market1501_test.query_done = False\n",
        "market1501_test.query_ptr = 0\n",
        "\n",
        "n_images = len(market1501_test.query_images)\n",
        "print(f'number of query images: {n_images}')\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "while not market1501_test.query_done:\n",
        "    images_t,labels_t = market1501_test.next_batch_query()\n",
        "\n",
        "    global_feat, local_feat = model(images_t)\n",
        "\n",
        "    query_feat.append(global_feat.cpu().detach())\n",
        "\n",
        "query_feat = torch.cat(query_feat)\n",
        "\n",
        "print('done')\n"
      ],
      "id": "QwgV033gTmXp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISpcnSZiTqey"
      },
      "outputs": [],
      "source": [
        "gallery_feat = []\n",
        "market1501_test.gallery_done = False\n",
        "market1501_test.gallery_ptr = 0\n",
        "\n",
        "n_images = len(market1501_test.gallery_images)\n",
        "print(f'number of gallery images: {n_images}')\n",
        "\n",
        "while not market1501_test.gallery_done:\n",
        "    images_t, labels_t = market1501_test.next_batch_gallery()\n",
        "\n",
        "    gobal_feat,local_feat = model(images_t)\n",
        "\n",
        "    gallery_feat.append(gobal_feat.cpu().detach())\n",
        "\n",
        "gallery_feat = torch.cat(gallery_feat)\n",
        "\n",
        "print('done')\n",
        "\n"
      ],
      "id": "ISpcnSZiTqey"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMuSE8b_Twj2"
      },
      "outputs": [],
      "source": [
        "# Calculate Distance\n",
        "dist_mat = euclidean_dist(query_feat,gallery_feat)"
      ],
      "id": "XMuSE8b_Twj2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87qTfkz_T0d5"
      },
      "outputs": [],
      "source": [
        "#Calculate Rank-1 and Rank-5\n",
        "\n",
        "cmc_score = cmc(dist_mat,market1501_test.query_ids,market1501_test.gallery_ids,market1501_test.query_cams,market1501_test.gallery_cams,separate_camera_set=False,sigle_gallery_shot=False,first_match_break=True)\n",
        "print(f'Rank-1 Score: {cmc_score[0]}')\n",
        "print(f'Rank-5 Score: {cmc_score[4]}')"
      ],
      "id": "87qTfkz_T0d5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEpVE8zGT3yK"
      },
      "outputs": [],
      "source": [
        "#Calculate mAP(mean Average Precision)\n",
        "mAP_score = mean_ap(dist_mat,market1501_test.query_ids,market1501_test.gallery_ids,market1501_test.query_cams,market1501_test.gallery_cams)\n",
        "print(f'mAP score: {mAP_score}')"
      ],
      "id": "IEpVE8zGT3yK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IUch9HA_BEF"
      },
      "outputs": [],
      "source": [],
      "id": "9IUch9HA_BEF"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}